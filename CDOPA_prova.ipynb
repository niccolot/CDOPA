{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import uuid #for generating guid code \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 200\n",
    "\n",
    "threshold = 7000 #intensity of pixel to ignore\n",
    "#number of depth images to keep, only a certain number contains the base gangli\n",
    "zmin = 20\n",
    "zmax = 30\n",
    "#portion of image which contains the interested part of brain scan\n",
    "xmin = 33\n",
    "xmax = 162\n",
    "ymin = 73\n",
    "ymax = 202\n",
    "\n",
    "deltaX = xmax-xmin\n",
    "deltaY = ymax-ymin\n",
    "deltaZ = zmax-zmin\n",
    "\n",
    "img_size = deltaX+1\n",
    "\n",
    "#during data augmentation the images will be shifted in the 4 directions of 5% and 10% of the width\n",
    "shifting_percentages = [img_size*0.05, img_size*0.1, -img_size*0.05, -img_size*0.1]\n",
    "\n",
    "background_percentage = 0.4\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "#model hyperparameters\n",
    "conv_depth_1 = 15 \n",
    "conv_depth_2 = 25 \n",
    "conv_depth_3 = 50 \n",
    "conv_depth_4 = 50 \n",
    "fc_nodes = 512\n",
    "\n",
    "drop_prob_1 = 0.25 \n",
    "drop_prob_2 = 0.5\n",
    "drop_prob_3 = 0.25 \n",
    "drop_prob_4 = 0.5\n",
    "l2_penalty = 1e-3\n",
    "\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.getcwd()\n",
    "\n",
    "try_num = 1\n",
    "try_dir = os.path.join(project_dir, 'try_{try_num}'.format(try_num=try_num))\n",
    "if os.path.exists(try_dir):\n",
    "    raise Exception(\"Directory name already used, update try_num\")\n",
    "else:\n",
    "    os.mkdir(try_dir)\n",
    "\n",
    "data_folder = os.path.join(project_dir, 'CDOPA_dataset')\n",
    "csv_path = os.path.join(project_dir, 'carbidopa.csv')\n",
    "\n",
    "hyper_params_filepath = os.path.join(try_dir, 'hyperparameters.txt')\n",
    "model_filepath = os.path.join(try_dir, 'model.h5')\n",
    "summary_filepath = os.path.join(try_dir, 'model_summary.txt')\n",
    "hist_csv_filepath = os.path.join(try_dir, 'history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATA LOADING FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def load_scan(path):\n",
    "    slices = [pydicom.read_file(os.path.join(path, s), force=True) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    try:\n",
    "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "    except:\n",
    "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "        \n",
    "    for s in slices:\n",
    "        s.SliceThickness = slice_thickness\n",
    "        \n",
    "    return slices\n",
    "\n",
    "\n",
    "def get_pixels(slices, rescale=False):\n",
    "    if rescale:\n",
    "        image = np.stack([s.pixel_array*(s.RescaleSlope*10) for s in slices])\n",
    "        return np.array(image,dtype=np.float32)\n",
    "    else:\n",
    "        image = np.stack([s.pixel_array for s in slices])\n",
    "        return np.array(image,dtype=np.int16)\n",
    "    \n",
    "\n",
    "def suv(slice, xmin, xmax, ymin, ymax):\n",
    "    x = np.linspace(xmin, xmax, xmax-xmin+1)\n",
    "    y = np.linspace(ymin, ymax, ymax-ymin+1)\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "    Z = slice[X.astype(np.int),Y.astype(np.int)]\n",
    "    return X,Y,Z\n",
    "\n",
    "\n",
    "def slicecutoff(slice, threshold):\n",
    "    slice[slice<threshold]=0.\n",
    "    return slice\n",
    "\n",
    "    \n",
    "def create_dataset(data_folder, rescale=False, threshold=threshold):\n",
    "\n",
    "    inumpatients = len(os.listdir(data_folder))#number of datapoints\n",
    "    X_DATA = np.zeros([inumpatients, deltaX+1, deltaY+1, deltaZ])#initially 'empty dataset'\n",
    "    patients = os.listdir(data_folder)#list all the folders containing the .dcm for each patient\n",
    "    patients.sort()\n",
    "    \n",
    "    for num_patient, patient in enumerate(patients):\n",
    "        patient_n = get_pixels(load_scan(os.path.join(data_folder, patients[num_patient])))\n",
    "\n",
    "        #reference central slice and mask\n",
    "        slice_central = patient_n[zmin+4]\n",
    "        slice_centralcut = slice_central[xmin:xmax+1, ymin:ymax+1]\n",
    "\n",
    "        if rescale:\n",
    "            threshold_roi = slice_centralcut.max()*background_percentage\n",
    "            threshold = threshold_roi\n",
    "\n",
    "        mask_central = slicecutoff(slice_centralcut, threshold)\n",
    "\n",
    "        for index, slice in enumerate(patient_n):\n",
    "            if index in range(zmin,zmax):\n",
    "                \n",
    "                slicecut = slice[xmin:xmax+1, ymin:ymax+1]\n",
    "                mask = slicecutoff(slicecut, threshold)\n",
    "\n",
    "                #here it makes sure the larger portion of the mask is considered background\n",
    "                #zeroing out the parts outsite the central region of interest\n",
    "                if index < zmin + 4:\n",
    "                    if (mask_central-mask).sum() > mask_central.sum():\n",
    "                        slicecut[True] = 0.\n",
    "                        mask[True] = 0.\n",
    "                if index > zmin +4:\n",
    "                    if abs((mask-mask_central)).sum() > mask_central.sum():\n",
    "                        slicecut[True] = 0.\n",
    "                        mask[True] = 0.\n",
    "                \n",
    "                X_DATA[num_patient,:,:,index-zmin] = slicecut\n",
    "    \n",
    "    return X_DATA\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATASET MANIPULATION FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "#shift param is how many pixel the image has to be shifted, \n",
    "# it has to be a signed flot for tanslation to the left or right\n",
    "\n",
    "\n",
    "def shift_image(image, shift_param, axis='x'):\n",
    "    \n",
    "    if axis=='x':\n",
    "        return tfa.image.translate(image, [shift_param,0])\n",
    "\n",
    "    if axis=='y':\n",
    "        return tfa.image.translate(image, [0, shift_param])\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Only x or y are valid axes\")\n",
    "\n",
    "\n",
    "def get_shifted_dataset(X_DATA, Y_D, Y_DATA, shifting_percentages):\n",
    "\n",
    "    axes = ['x', 'y']\n",
    "\n",
    "    augmented_dataset = X_DATA\n",
    "\n",
    "    #augmented_dataset = np.zeros([2*len(shifting_percentages), img_size, img_size, deltaZ])\n",
    "\n",
    "    for numpatient in range(len(X_DATA)):\n",
    "\n",
    "        for shift_perc in shifting_percentages:\n",
    "\n",
    "            for axis in axes:\n",
    "            \n",
    "                shifted_image = shift_image(X_DATA[numpatient, :, :, :], shift_param=shift_perc, axis=axis)\n",
    "                #shifted_image_minus = shift_image(X_DATA[numpatient, :, :, :], shift_param=-shift_perc, axis=axis)\n",
    "                shifted_image = np.expand_dims(shifted_image, axis=0)\n",
    "                #shifted_image_minus = np.expand_dims(shifted_image_minus, axis=0)\n",
    "                augmented_dataset = np.append(augmented_dataset, shifted_image, axis=0)\n",
    "                #augmented_dataset = np.append(augmented_dataset, shifted_image_minus, axis=0)\n",
    "\n",
    "                guid = uuid.uuid4()\n",
    "                current_label = Y_DATA[numpatient]\n",
    "                last_index = len(X_DATA)\n",
    "                \n",
    "                Y_D = np.append(Y_D, np.empty([1,2]), axis=0)\n",
    "                Y_DATA = np.append(Y_DATA, np.empty([1,1]), axis=0)\n",
    "                Y_D[last_index + numpatient, :] = (str(guid), current_label)\n",
    "                Y_DATA[last_index + numpatient] = current_label\n",
    "\n",
    "    return augmented_dataset, Y_D, Y_DATA\n",
    "\n",
    "\n",
    "def prep_dataset(X_DATA, Y_DATA, num_classes):\n",
    "\n",
    "    X_DATA = X_DATA.astype('float32')\n",
    "    X_DATA = np.expand_dims(X_DATA, axis=4)#axis to store channel info\n",
    "\n",
    "    Y_DATA[Y_DATA=='N'] = 0.\n",
    "    Y_DATA[Y_DATA=='T'] = 0.\n",
    "    Y_DATA[Y_DATA=='P'] = 1.\n",
    "    Y_DATA = keras.utils.to_categorical(Y_DATA, num_classes)\n",
    "\n",
    "    return X_DATA, Y_DATA\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODEL BUILDING FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def conv_pool_drop_block(\n",
    "    input,\n",
    "    num_filters,\n",
    "    l2_penalty,\n",
    "    drop_rate,\n",
    "    kernel_size=3,\n",
    "    padding='same',\n",
    "    kernel_initializer='he_uniform',\n",
    "    kernel_regularizer=regularizers.L2(l2_penalty),\n",
    "    activation='relu',\n",
    "    pool_size=2,\n",
    "):\n",
    "\n",
    "    x = layers.Conv3D(\n",
    "        num_filters, \n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        activation=activation\n",
    "        )(input)\n",
    "\n",
    "    x = layers.MaxPooling3D(pool_size=pool_size)(x)\n",
    "    x = layers.Dropout(drop_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def fc_block(\n",
    "    input,\n",
    "    num_layers,\n",
    "    fc_nodes,\n",
    "    drop_rate,\n",
    "    l2_penalty,\n",
    "    kernel_regularizer=regularizers.L2(l2_penalty),\n",
    "    activation='relu',\n",
    "\n",
    "):\n",
    "    x = layers.Flatten()(input)\n",
    "\n",
    "    for fc_layers in range(num_layers):\n",
    "        x = layers.Dense(\n",
    "            fc_nodes,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            )(x)\n",
    "        x = layers.Dropout(drop_rate)(x)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "\n",
    "def build_model(\n",
    "    img_size,\n",
    "    num_classes,\n",
    "):\n",
    "    input = keras.Input(shape=(img_size, img_size, deltaZ, 1))\n",
    "    x = layers.Rescaling(scale=1/125.5, offset=-1)(input)\n",
    "\n",
    "    x = conv_pool_drop_block(x, num_filters=conv_depth_1, l2_penalty=l2_penalty, drop_rate=drop_prob_1)\n",
    "    x = conv_pool_drop_block(x, num_filters=conv_depth_2, l2_penalty=l2_penalty, drop_rate=drop_prob_2)\n",
    "    x = conv_pool_drop_block(x, num_filters=conv_depth_3, l2_penalty=l2_penalty, drop_rate=drop_prob_3)\n",
    "    x = layers.Conv3D(conv_depth_4, (3,3,3), padding='same', kernel_initializer='he_uniform', kernel_regularizer=regularizers.L2(l2_penalty), activation='relu')(x)\n",
    "    x = layers.Dropout(drop_prob_4)(x)\n",
    "\n",
    "    x = fc_block(x, num_layers=2, fc_nodes=fc_nodes, drop_rate=drop_prob_4, l2_penalty=l2_penalty)\n",
    "    \n",
    "    output = layers.Dense(num_classes, kernel_regularizer=regularizers.L2(l2_penalty), activation='softmax')(x)\n",
    "\n",
    "    return keras.Model(inputs=input, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(csv_path)\n",
    "df_csv['GUID']= df_csv['GUID'].astype(str)\n",
    "df_sorted = df_csv.sort_values('GUID')\n",
    "df_sorted = df_sorted.iloc[0:87,:]#in the csv there are more datapoints than in the CDOPA_dataset folder\n",
    "\n",
    "Y_D = np.array(df_sorted[['GUID','LABEL']])\n",
    "Y_DATA = np.array(df_sorted[['LABEL']])\n",
    "X_DATA = create_dataset(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 130, 130, 10)\n",
      "(87, 2)\n",
      "(87, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_DATA.shape)\n",
    "print(Y_D.shape)\n",
    "print(Y_DATA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DATA, X_DATA_TEST, Y_DATA, Y_DATA_TEST, Y_D, Y_D_TEST = train_test_split(X_DATA, Y_DATA, Y_D, test_size=0.30, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 130, 130, 10)\n",
      "(60, 2)\n",
      "(60, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_DATA.shape)\n",
    "print(Y_D.shape)\n",
    "print(Y_DATA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 130, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_DATA[0, :, :, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DATA_NEW, Y_D_NEW, Y_DATA_NEW = get_shifted_dataset(X_DATA, Y_D, Y_DATA, shifting_percentages=shifting_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 130, 130, 10)\n",
      "(540, 2)\n",
      "(540, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_DATA_NEW.shape)\n",
    "print(Y_D_NEW.shape)\n",
    "print(Y_DATA_NEW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DATA_NEW, Y_DATA_NEW = prep_dataset(X_DATA_NEW, Y_DATA_NEW, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 130, 130, 10, 1)\n",
      "(540, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_DATA_NEW.shape)\n",
    "print(Y_DATA_NEW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DATA_TRAIN, X_DATA_VAL, Y_DATA_TRAIN, Y_DATA_VAL, Y_D_TRAIN, Y_D_VAL = train_test_split(X_DATA_NEW, Y_DATA_NEW, Y_D_NEW, test_size=0.20, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_DATA_TRAIN.shape)\n",
    "print(X_DATA_VAL.shape)\n",
    "print(Y_D_TRAIN.shape)\n",
    "print(Y_D_VAL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "model = build_model(img_size=img_size, num_classes=num_classes)\n",
    "\n",
    "with open(summary_filepath, 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        patience=25,\n",
    "        monitor='val_loss'\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_filepath,\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x = X_DATA_TRAIN,\n",
    "    y = Y_DATA_TRAIN,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_DATA_VAL, Y_DATA_VAL),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "with open(hist_csv_filepath, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f84fa9da9fc9effead0dc3a0ddec46df01cd77a4c130e3b9442917017e134d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
